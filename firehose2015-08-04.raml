#%RAML 1.0
title: Amazon Kinesis Firehose
version: '2015-08-04'
description: '  Amazon Kinesis Firehose is a fully-managed service that delivers real-time
  streaming data to destinations such as Amazon Simple Storage Service (Amazon S3),
  Amazon Elasticsearch Service (Amazon ES), and Amazon Redshift.'
mediaType:
- application/json
uses:
  commons: https://raw.githubusercontent.com/apiregistry/commons/master/commons.raml
  extras: https://raw.githubusercontent.com/apiregistry/typesExtras/master/typeExtras.raml
types:
  AWSKMSKeyARN:
    type: string
    pattern: arn:.*
    minLength: 1
    maxLength: 512
  BooleanObject:
    type: boolean
  BucketARN:
    type: string
    pattern: arn:.*
    minLength: 1
    maxLength: 2048
  ClusterJDBCURL:
    type: string
    pattern: jdbc:(redshift|postgresql)://((?!-)[A-Za-z0-9-]{1,63}(?<!-)\.)+redshift\.amazonaws\.com:\d{1,5}/[a-zA-Z0-9_$]+
    minLength: 1
  CompressionFormat:
    type: string
    enum:
    - UNCOMPRESSED
    - GZIP
    - ZIP
    - Snappy
  CopyOptions:
    type: string
  Data:
    type: file
    minLength: 0
    maxLength: 1024000
  DataTableColumns:
    type: string
  DataTableName:
    type: string
    minLength: 1
  DeliveryStreamARN:
    type: string
  DeliveryStreamName:
    type: string
    pattern: '[a-zA-Z0-9_.-]+'
    minLength: 1
    maxLength: 64
  DeliveryStreamNameList:
    type: array
    items:
      type: DeliveryStreamName
  DeliveryStreamStatus:
    type: string
    enum:
    - CREATING
    - DELETING
    - ACTIVE
  DeliveryStreamVersionId:
    type: string
    pattern: '[0-9]+'
    minLength: 1
    maxLength: 50
  DescribeDeliveryStreamInputLimit:
    type: integer
    format: int
    minimum: 1
    maximum: 10000
  DestinationDescriptionList:
    type: array
    items:
      type: DestinationDescription
  DestinationId:
    type: string
    minLength: 1
    maxLength: 100
  ElasticsearchBufferingIntervalInSeconds:
    type: integer
    format: int
    minimum: 60
    maximum: 900
  ElasticsearchBufferingSizeInMBs:
    type: integer
    format: int
    minimum: 1
    maximum: 100
  ElasticsearchDomainARN:
    type: string
    pattern: arn:.*
    minLength: 1
    maxLength: 512
  ElasticsearchIndexName:
    type: string
    minLength: 1
    maxLength: 80
  ElasticsearchIndexRotationPeriod:
    type: string
    enum:
    - NoRotation
    - OneHour
    - OneDay
    - OneWeek
    - OneMonth
  ElasticsearchRetryDurationInSeconds:
    type: integer
    format: int
    minimum: 0
    maximum: 7200
  ElasticsearchS3BackupMode:
    type: string
    enum:
    - FailedDocumentsOnly
    - AllDocuments
  ElasticsearchTypeName:
    type: string
    minLength: 1
    maxLength: 100
  ErrorCode:
    type: string
  ErrorMessage:
    type: string
  IntervalInSeconds:
    type: integer
    format: int
    minimum: 60
    maximum: 900
  ListDeliveryStreamsInputLimit:
    type: integer
    format: int
    minimum: 1
    maximum: 10000
  LogGroupName:
    type: string
  LogStreamName:
    type: string
  NoEncryptionConfig:
    type: string
    enum:
    - NoEncryption
  NonNegativeIntegerObject:
    type: integer
    format: int
    minimum: 0
  Password:
    type: string
    minLength: 6
  Prefix:
    type: string
  PutRecordBatchRequestEntryList:
    type: array
    minItems: 1
    maxItems: 500
    items:
      type: Record
  PutRecordBatchResponseEntryList:
    type: array
    minItems: 1
    maxItems: 500
    items:
      type: PutRecordBatchResponseEntry
  PutResponseRecordId:
    type: string
    minLength: 1
  RedshiftRetryDurationInSeconds:
    type: integer
    format: int
    minimum: 0
    maximum: 7200
  RoleARN:
    type: string
    pattern: arn:.*
    minLength: 1
    maxLength: 512
  SizeInMBs:
    type: integer
    format: int
    minimum: 1
    maximum: 128
  Timestamp:
    type: datetime
  Username:
    type: string
    minLength: 1
  BufferingHints:
    type: object
    properties:
      SizeInMBs?:
        type: SizeInMBs
        description: <p>Buffer incoming data to the specified size, in MBs, before delivering it to the destination. The default value is 5.</p> <p>We recommend setting SizeInMBs to a value greater than the amount of data you typically ingest into the delivery stream in 10 seconds. For example, if you typically ingest data at 1 MB/sec set SizeInMBs to be 10 MB or higher.</p>
      IntervalInSeconds?:
        type: IntervalInSeconds
        description: <p>Buffer incoming data for the specified period of time, in seconds, before delivering it to the destination. The default value is 300.</p>
    description: <p>Describes hints for the buffering to perform before delivering data to the destination. Please note that these options are treated as hints, and therefore Firehose may choose to use different values when it is optimal.</p>
  CloudWatchLoggingOptions:
    type: object
    properties:
      Enabled?:
        type: BooleanObject
        description: <p>Enables or disables CloudWatch logging.</p>
      LogGroupName?:
        type: LogGroupName
        description: <p>The CloudWatch group name for logging. This value is required if Enabled is true.</p>
      LogStreamName?:
        type: LogStreamName
        description: <p>The CloudWatch log stream name for logging. This value is required if Enabled is true.</p>
    description: <p>Describes CloudWatch logging options for your delivery stream.</p>
  CopyCommand:
    type: object
    properties:
      DataTableName:
        type: DataTableName
        description: <p>The name of the target table. The table must already exist in the database.</p>
      DataTableColumns?:
        type: DataTableColumns
        description: <p>A comma-separated list of column names.</p>
      CopyOptions?:
        type: CopyOptions
        description: <p>Optional parameters to use with the Amazon Redshift <code>COPY</code> command. For more information, see the "Optional Parameters" section of <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html">Amazon Redshift COPY command</a>. Some possible examples that would apply to Firehose are as follows.</p> <p> <code>delimiter '\t' lzop;</code> - fields are delimited with "\t" (TAB character) and compressed using lzop.</p> <p> <code>delimiter '|</code> - fields are delimited with "|" (this is the default delimiter).</p> <p> <code>delimiter '|' escape</code> - the delimiter should be escaped.</p> <p> <code>fixedwidth 'venueid:3,venuename:25,venuecity:12,venuestate:2,venueseats:6'</code> - fields are fixed width in the source, with each width specified after every column in the table.</p> <p> <code>JSON 's3://mybucket/jsonpaths.txt'</code> - data is in JSON format, and the path specified is the format of the data.</p> <p>For more examples, see <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_COPY_command_examples.html">Amazon Redshift COPY command examples</a>.</p>
    description: <p>Describes a <code>COPY</code> command for Amazon Redshift.</p>
  CreateDeliveryStreamInput:
    type: object
    properties:
      DeliveryStreamName:
        type: DeliveryStreamName
        description: <p>The name of the delivery stream.</p>
      S3DestinationConfiguration?:
        type: S3DestinationConfiguration
        description: <p>The destination in Amazon S3. This value must be specified if <b>ElasticsearchDestinationConfiguration</b> or <b>RedshiftDestinationConfiguration</b> is specified (see restrictions listed above).</p>
      RedshiftDestinationConfiguration?:
        type: RedshiftDestinationConfiguration
        description: <p>The destination in Amazon Redshift. This value cannot be specified if Amazon S3 or Amazon Elasticsearch is the desired destination (see restrictions listed above).</p>
      ElasticsearchDestinationConfiguration?:
        type: ElasticsearchDestinationConfiguration
        description: <p>The destination in Amazon ES. This value cannot be specified if Amazon S3 or Amazon Redshift is the desired destination (see restrictions listed above).</p>
    description: <p>Contains the parameters for <a>CreateDeliveryStream</a>.</p>
  CreateDeliveryStreamOutput:
    type: object
    properties:
      DeliveryStreamARN?:
        type: DeliveryStreamARN
        description: <p>The ARN of the delivery stream.</p>
    description: <p>Contains the output of <a>CreateDeliveryStream</a>.</p>
  DeleteDeliveryStreamInput:
    type: object
    properties:
      DeliveryStreamName:
        type: DeliveryStreamName
        description: <p>The name of the delivery stream.</p>
    description: <p>Contains the parameters for <a>DeleteDeliveryStream</a>.</p>
  DeleteDeliveryStreamOutput:
    type: object
    description: <p>Contains the output of <a>DeleteDeliveryStream</a>.</p>
  DeliveryStreamDescription:
    type: object
    properties:
      DeliveryStreamName:
        type: DeliveryStreamName
        description: <p>The name of the delivery stream.</p>
      DeliveryStreamARN:
        type: DeliveryStreamARN
        description: <p>The Amazon Resource Name (ARN) of the delivery stream.</p>
      DeliveryStreamStatus:
        type: DeliveryStreamStatus
        description: <p>The status of the delivery stream.</p>
      VersionId:
        type: DeliveryStreamVersionId
        description: <p>Used when calling the <a>UpdateDestination</a> operation. Each time the destination is updated for the delivery stream, the VersionId is changed, and the current VersionId is required when updating the destination. This is so that the service knows it is applying the changes to the correct version of the delivery stream.</p>
      CreateTimestamp?:
        type: Timestamp
        description: <p>The date and time that the delivery stream was created.</p>
      LastUpdateTimestamp?:
        type: Timestamp
        description: <p>The date and time that the delivery stream was last updated.</p>
      Destinations:
        type: DestinationDescriptionList
        description: <p>The destinations.</p>
      HasMoreDestinations:
        type: BooleanObject
        description: <p>Indicates whether there are more destinations available to list.</p>
    description: <p>Contains information about a delivery stream.</p>
  DescribeDeliveryStreamInput:
    type: object
    properties:
      DeliveryStreamName:
        type: DeliveryStreamName
        description: <p>The name of the delivery stream.</p>
      Limit?:
        type: DescribeDeliveryStreamInputLimit
        description: <p>The limit on the number of destinations to return. Currently, you can have one destination per delivery stream.</p>
      ExclusiveStartDestinationId?:
        type: DestinationId
        description: <p>Specifies the destination ID to start returning the destination information. Currently Firehose supports one destination per delivery stream.</p>
    description: <p>Contains the parameters for <a>DescribeDeliveryStream</a>.</p>
  DescribeDeliveryStreamOutput:
    type: object
    properties:
      DeliveryStreamDescription:
        type: DeliveryStreamDescription
        description: <p>Information about the delivery stream.</p>
    description: <p>Contains the output of <a>DescribeDeliveryStream</a>.</p>
  DestinationDescription:
    type: object
    properties:
      DestinationId:
        type: DestinationId
        description: <p>The ID of the destination.</p>
      S3DestinationDescription?:
        type: S3DestinationDescription
        description: <p>The Amazon S3 destination.</p>
      RedshiftDestinationDescription?:
        type: RedshiftDestinationDescription
        description: <p>The destination in Amazon Redshift.</p>
      ElasticsearchDestinationDescription?:
        type: ElasticsearchDestinationDescription
        description: <p>The destination in Amazon ES.</p>
    description: <p>Describes the destination for a delivery stream.</p>
  ElasticsearchBufferingHints:
    type: object
    properties:
      IntervalInSeconds?:
        type: ElasticsearchBufferingIntervalInSeconds
        description: <p>Buffer incoming data for the specified period of time, in seconds, before delivering it to the destination. The default value is 300 (5 minutes).</p>
      SizeInMBs?:
        type: ElasticsearchBufferingSizeInMBs
        description: <p>Buffer incoming data to the specified size, in MBs, before delivering it to the destination. The default value is 5.</p> <p>We recommend setting <b>SizeInMBs</b> to a value greater than the amount of data you typically ingest into the delivery stream in 10 seconds. For example, if you typically ingest data at 1 MB/sec, set <b>SizeInMBs</b> to be 10 MB or higher.</p>
    description: <p>Describes the buffering to perform before delivering data to the Amazon ES destination.</p>
  ElasticsearchDestinationConfiguration:
    type: object
    properties:
      RoleARN:
        type: RoleARN
        description: <p>The ARN of the IAM role to be assumed by Firehose for calling the Amazon ES Configuration API and for indexing documents. For more information, see <a href="http://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-s3">Amazon S3 Bucket Access</a>.</p>
      DomainARN:
        type: ElasticsearchDomainARN
        description: <p>The ARN of the Amazon ES domain. The IAM role must have permission for <code>DescribeElasticsearchDomain</code>, <code>DescribeElasticsearchDomains</code> , and <code>DescribeElasticsearchDomainConfig</code> after assuming <b>RoleARN</b>.</p>
      IndexName:
        type: ElasticsearchIndexName
        description: <p>The Elasticsearch index name.</p>
      TypeName:
        type: ElasticsearchTypeName
        description: <p>The Elasticsearch type name.</p>
      IndexRotationPeriod?:
        type: ElasticsearchIndexRotationPeriod
        description: <p>The Elasticsearch index rotation period. Index rotation appends a timestamp to the IndexName to facilitate expiration of old data. For more information, see <a href="http://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html#es-index-rotation">Index Rotation for Amazon Elasticsearch Service Destination</a>. Default value is <code>OneDay</code>.</p>
      BufferingHints?:
        type: ElasticsearchBufferingHints
        description: <p>Buffering options. If no value is specified, <b>ElasticsearchBufferingHints</b> object default values are used. </p>
      RetryOptions?:
        type: ElasticsearchRetryOptions
        description: <p>Configures retry behavior in the event that Firehose is unable to deliver documents to Amazon ES. Default value is 300 (5 minutes).</p>
      S3BackupMode?:
        type: ElasticsearchS3BackupMode
        description: <p>Defines how documents should be delivered to Amazon S3. When set to FailedDocumentsOnly, Firehose writes any documents that could not be indexed to the configured Amazon S3 destination, with elasticsearch-failed/ appended to the key prefix. When set to AllDocuments, Firehose delivers all incoming records to Amazon S3, and also writes failed documents with elasticsearch-failed/ appended to the prefix. For more information, see <a href="http://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html#es-s3-backup">Amazon S3 Backup for Amazon Elasticsearch Service Destination</a>. Default value is FailedDocumentsOnly.</p>
      S3Configuration:
        type: S3DestinationConfiguration
      CloudWatchLoggingOptions?:
        type: CloudWatchLoggingOptions
        description: <p>Describes CloudWatch logging options for your delivery stream.</p>
    description: <p>Describes the configuration of a destination in Amazon ES.</p>
  ElasticsearchDestinationDescription:
    type: object
    properties:
      RoleARN?:
        type: RoleARN
        description: <p>The ARN of the AWS credentials.</p>
      DomainARN?:
        type: ElasticsearchDomainARN
        description: <p>The ARN of the Amazon ES domain.</p>
      IndexName?:
        type: ElasticsearchIndexName
        description: <p>The Elasticsearch index name.</p>
      TypeName?:
        type: ElasticsearchTypeName
        description: <p>The Elasticsearch type name.</p>
      IndexRotationPeriod?:
        type: ElasticsearchIndexRotationPeriod
        description: <p>The Elasticsearch index rotation period</p>
      BufferingHints?:
        type: ElasticsearchBufferingHints
        description: <p>Buffering options.</p>
      RetryOptions?:
        type: ElasticsearchRetryOptions
        description: <p>Elasticsearch retry options.</p>
      S3BackupMode?:
        type: ElasticsearchS3BackupMode
        description: <p>Amazon S3 backup mode.</p>
      S3DestinationDescription?:
        type: S3DestinationDescription
      CloudWatchLoggingOptions?:
        type: CloudWatchLoggingOptions
        description: <p>CloudWatch logging options.</p>
    description: <p>The destination description in Amazon ES.</p>
  ElasticsearchDestinationUpdate:
    type: object
    properties:
      RoleARN?:
        type: RoleARN
        description: <p>The ARN of the IAM role to be assumed by Firehose for calling the Amazon ES Configuration API and for indexing documents. For more information, see <a href="http://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-s3">Amazon S3 Bucket Access</a>.</p>
      DomainARN?:
        type: ElasticsearchDomainARN
        description: <p>The ARN of the Amazon ES domain. The IAM role must have permission for DescribeElasticsearchDomain, DescribeElasticsearchDomains , and DescribeElasticsearchDomainConfig after assuming <b>RoleARN</b>.</p>
      IndexName?:
        type: ElasticsearchIndexName
        description: <p>The Elasticsearch index name.</p>
      TypeName?:
        type: ElasticsearchTypeName
        description: <p>The Elasticsearch type name.</p>
      IndexRotationPeriod?:
        type: ElasticsearchIndexRotationPeriod
        description: <p>The Elasticsearch index rotation period. Index rotation appends a timestamp to the IndexName to facilitate the expiration of old data. For more information, see <a href="http://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html#es-index-rotation">Index Rotation for Amazon Elasticsearch Service Destination</a>. Default value is <code>OneDay</code>.</p>
      BufferingHints?:
        type: ElasticsearchBufferingHints
        description: <p>Buffering options. If no value is specified, <b>ElasticsearchBufferingHints</b> object default values are used. </p>
      RetryOptions?:
        type: ElasticsearchRetryOptions
        description: <p>Configures retry behavior in the event that Firehose is unable to deliver documents to Amazon ES. Default value is 300 (5 minutes).</p>
      S3Update?:
        type: S3DestinationUpdate
      CloudWatchLoggingOptions?:
        type: CloudWatchLoggingOptions
        description: <p>Describes CloudWatch logging options for your delivery stream.</p>
    description: <p>Describes an update for a destination in Amazon ES.</p>
  ElasticsearchRetryOptions:
    type: object
    properties:
      DurationInSeconds?:
        type: ElasticsearchRetryDurationInSeconds
        description: <p>After an initial failure to deliver to Amazon ES, the total amount of time during which Firehose re-attempts delivery (including the first attempt). After this time has elapsed, the failed documents are written to Amazon S3. Default value is 300 seconds (5 minutes). A value of 0 (zero) results in no retries.</p>
    description: <p>Configures retry behavior in the event that Firehose is unable to deliver documents to Amazon ES.</p>
  EncryptionConfiguration:
    type: object
    properties:
      NoEncryptionConfig?:
        type: NoEncryptionConfig
        description: <p>Specifically override existing encryption information to ensure no encryption is used.</p>
      KMSEncryptionConfig?:
        type: KMSEncryptionConfig
        description: <p>The encryption key.</p>
    description: <p>Describes the encryption for a destination in Amazon S3.</p>
  KMSEncryptionConfig:
    type: object
    properties:
      AWSKMSKeyARN:
        type: AWSKMSKeyARN
        description: <p>The ARN of the encryption key. Must belong to the same region as the destination Amazon S3 bucket.</p>
    description: <p>Describes an encryption key for a destination in Amazon S3.</p>
  ListDeliveryStreamsInput:
    type: object
    properties:
      Limit?:
        type: ListDeliveryStreamsInputLimit
        description: <p>The maximum number of delivery streams to list.</p>
      ExclusiveStartDeliveryStreamName?:
        type: DeliveryStreamName
        description: <p>The name of the delivery stream to start the list with.</p>
    description: <p>Contains the parameters for <a>ListDeliveryStreams</a>.</p>
  ListDeliveryStreamsOutput:
    type: object
    properties:
      DeliveryStreamNames:
        type: DeliveryStreamNameList
        description: <p>The names of the delivery streams.</p>
      HasMoreDeliveryStreams:
        type: BooleanObject
        description: <p>Indicates whether there are more delivery streams available to list.</p>
    description: <p>Contains the output of <a>ListDeliveryStreams</a>.</p>
  PutRecordBatchInput:
    type: object
    properties:
      DeliveryStreamName:
        type: DeliveryStreamName
        description: <p>The name of the delivery stream.</p>
      Records:
        type: PutRecordBatchRequestEntryList
        description: <p>One or more records.</p>
    description: <p>Contains the parameters for <a>PutRecordBatch</a>.</p>
  PutRecordBatchOutput:
    type: object
    properties:
      FailedPutCount:
        type: NonNegativeIntegerObject
        description: <p>The number of unsuccessfully written records.</p>
      RequestResponses:
        type: PutRecordBatchResponseEntryList
        description: <p>The results for the individual records. The index of each element matches the same index in which records were sent.</p>
    description: <p>Contains the output of <a>PutRecordBatch</a>.</p>
  PutRecordBatchResponseEntry:
    type: object
    properties:
      RecordId?:
        type: PutResponseRecordId
        description: <p>The ID of the record.</p>
      ErrorCode?:
        type: ErrorCode
        description: <p>The error code for an individual record result.</p>
      ErrorMessage?:
        type: ErrorMessage
        description: <p>The error message for an individual record result.</p>
    description: <p>Contains the result for an individual record from a <a>PutRecordBatch</a> request. If the record is successfully added to your delivery stream, it receives a record ID. If the record fails to be added to your delivery stream, the result includes an error code and an error message.</p>
  PutRecordInput:
    type: object
    properties:
      DeliveryStreamName:
        type: DeliveryStreamName
        description: <p>The name of the delivery stream.</p>
      Record:
        type: Record
        description: <p>The record.</p>
    description: <p>Contains the parameters for <a>PutRecord</a>.</p>
  PutRecordOutput:
    type: object
    properties:
      RecordId:
        type: PutResponseRecordId
        description: <p>The ID of the record.</p>
    description: <p>Contains the output of <a>PutRecord</a>.</p>
  Record:
    type: object
    properties:
      Data:
        type: Data
        description: <p>The data blob, which is base64-encoded when the blob is serialized. The maximum size of the data blob, before base64-encoding, is 1,000 KB.</p>
    description: <p>The unit of data in a delivery stream.</p>
  RedshiftDestinationConfiguration:
    type: object
    properties:
      RoleARN:
        type: RoleARN
        description: <p>The ARN of the AWS credentials.</p>
      ClusterJDBCURL:
        type: ClusterJDBCURL
        description: <p>The database connection string.</p>
      CopyCommand:
        type: CopyCommand
        description: <p>The <code>COPY</code> command.</p>
      Username:
        type: Username
        description: <p>The name of the user.</p>
      Password:
        type: Password
        description: <p>The user password.</p>
      RetryOptions?:
        type: RedshiftRetryOptions
        description: <p>Configures retry behavior in the event that Firehose is unable to deliver documents to Amazon Redshift. Default value is 3600 (60 minutes).</p>
      S3Configuration:
        type: S3DestinationConfiguration
        description: <p>The S3 configuration for the intermediate location from which Amazon Redshift obtains data. Restrictions are described in the topic for <a>CreateDeliveryStream</a>.</p> <p>The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot be specified in <b>RedshiftDestinationConfiguration.S3Configuration</b> because the Amazon Redshift <code>COPY</code> operation that reads from the S3 bucket doesn't support these compression formats.</p>
      CloudWatchLoggingOptions?:
        type: CloudWatchLoggingOptions
        description: <p>Describes CloudWatch logging options for your delivery stream.</p>
    description: <p>Describes the configuration of a destination in Amazon Redshift.</p>
  RedshiftDestinationDescription:
    type: object
    properties:
      RoleARN:
        type: RoleARN
        description: <p>The ARN of the AWS credentials.</p>
      ClusterJDBCURL:
        type: ClusterJDBCURL
        description: <p>The database connection string.</p>
      CopyCommand:
        type: CopyCommand
        description: <p>The <code>COPY</code> command.</p>
      Username:
        type: Username
        description: <p>The name of the user.</p>
      RetryOptions?:
        type: RedshiftRetryOptions
        description: <p>Configures retry behavior in the event that Firehose is unable to deliver documents to Amazon Redshift. Default value is 3600 (60 minutes).</p>
      S3DestinationDescription:
        type: S3DestinationDescription
        description: <p>The Amazon S3 destination.</p>
      CloudWatchLoggingOptions?:
        type: CloudWatchLoggingOptions
        description: <p>Describes CloudWatch logging options for your delivery stream.</p>
    description: <p>Describes a destination in Amazon Redshift.</p>
  RedshiftDestinationUpdate:
    type: object
    properties:
      RoleARN?:
        type: RoleARN
        description: <p>The ARN of the AWS credentials.</p>
      ClusterJDBCURL?:
        type: ClusterJDBCURL
        description: <p>The database connection string.</p>
      CopyCommand?:
        type: CopyCommand
        description: <p>The <code>COPY</code> command.</p>
      Username?:
        type: Username
        description: <p>The name of the user.</p>
      Password?:
        type: Password
        description: <p>The user password.</p>
      RetryOptions?:
        type: RedshiftRetryOptions
        description: <p>Configures retry behavior in the event that Firehose is unable to deliver documents to Amazon Redshift. Default value is 3600 (60 minutes).</p>
      S3Update?:
        type: S3DestinationUpdate
        description: <p>The Amazon S3 destination.</p> <p>The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot be specified in <b>RedshiftDestinationUpdate.S3Update</b> because the Amazon Redshift <code>COPY</code> operation that reads from the S3 bucket doesn't support these compression formats.</p>
      CloudWatchLoggingOptions?:
        type: CloudWatchLoggingOptions
        description: <p>Describes CloudWatch logging options for your delivery stream.</p>
    description: <p>Describes an update for a destination in Amazon Redshift.</p>
  RedshiftRetryOptions:
    type: object
    properties:
      DurationInSeconds?:
        type: RedshiftRetryDurationInSeconds
        description: <p>The length of time during which Firehose retries delivery after a failure, starting from the initial request and including the first attempt. The default value is 3600 seconds (60 minutes). Firehose does not retry if the value of <code>DurationInSeconds</code> is 0 (zero) or if the first delivery attempt takes longer than the current value.</p>
    description: <p>Configures retry behavior in the event that Firehose is unable to deliver documents to Amazon Redshift.</p>
  S3DestinationConfiguration:
    type: object
    properties:
      RoleARN:
        type: RoleARN
        description: <p>The ARN of the AWS credentials.</p>
      BucketARN:
        type: BucketARN
        description: <p>The ARN of the S3 bucket.</p>
      Prefix?:
        type: Prefix
        description: <p>The "YYYY/MM/DD/HH" time format prefix is automatically used for delivered S3 files. You can specify an extra prefix to be added in front of the time format prefix. Note that if the prefix ends with a slash, it appears as a folder in the S3 bucket. For more information, see <a href="http://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html">Amazon S3 Object Name Format</a> in the <a href="http://docs.aws.amazon.com/firehose/latest/dev/">Amazon Kinesis Firehose Developer Guide</a>.</p>
      BufferingHints?:
        type: BufferingHints
        description: <p>The buffering option. If no value is specified, <b>BufferingHints</b> object default values are used.</p>
      CompressionFormat?:
        type: CompressionFormat
        description: <p>The compression format. If no value is specified, the default is <code>UNCOMPRESSED</code>.</p> <p>The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot be specified for Amazon Redshift destinations because they are not supported by the Amazon Redshift <code>COPY</code> operation that reads from the S3 bucket.</p>
      EncryptionConfiguration?:
        type: EncryptionConfiguration
        description: <p>The encryption configuration. If no value is specified, the default is no encryption.</p>
      CloudWatchLoggingOptions?:
        type: CloudWatchLoggingOptions
        description: <p>Describes CloudWatch logging options for your delivery stream.</p>
    description: <p>Describes the configuration of a destination in Amazon S3.</p>
  S3DestinationDescription:
    type: object
    properties:
      RoleARN:
        type: RoleARN
        description: <p>The ARN of the AWS credentials.</p>
      BucketARN:
        type: BucketARN
        description: <p>The ARN of the S3 bucket.</p>
      Prefix?:
        type: Prefix
        description: <p>The "YYYY/MM/DD/HH" time format prefix is automatically used for delivered S3 files. You can specify an extra prefix to be added in front of the time format prefix. Note that if the prefix ends with a slash, it appears as a folder in the S3 bucket. For more information, see <a href="http://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html">Amazon S3 Object Name Format</a> in the <a href="http://docs.aws.amazon.com/firehose/latest/dev/">Amazon Kinesis Firehose Developer Guide</a>.</p>
      BufferingHints:
        type: BufferingHints
        description: <p>The buffering option. If no value is specified, <b>BufferingHints</b> object default values are used.</p>
      CompressionFormat:
        type: CompressionFormat
        description: <p>The compression format. If no value is specified, the default is <code>NOCOMPRESSION</code>.</p>
      EncryptionConfiguration:
        type: EncryptionConfiguration
        description: <p>The encryption configuration. If no value is specified, the default is no encryption.</p>
      CloudWatchLoggingOptions?:
        type: CloudWatchLoggingOptions
        description: <p>Describes CloudWatch logging options for your delivery stream.</p>
    description: <p>Describes a destination in Amazon S3.</p>
  S3DestinationUpdate:
    type: object
    properties:
      RoleARN?:
        type: RoleARN
        description: <p>The ARN of the AWS credentials.</p>
      BucketARN?:
        type: BucketARN
        description: <p>The ARN of the S3 bucket.</p>
      Prefix?:
        type: Prefix
        description: <p>The "YYYY/MM/DD/HH" time format prefix is automatically used for delivered S3 files. You can specify an extra prefix to be added in front of the time format prefix. Note that if the prefix ends with a slash, it appears as a folder in the S3 bucket. For more information, see <a href="http://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html">Amazon S3 Object Name Format</a> in the <a href="http://docs.aws.amazon.com/firehose/latest/dev/">Amazon Kinesis Firehose Developer Guide</a>.</p>
      BufferingHints?:
        type: BufferingHints
        description: <p>The buffering option. If no value is specified, <b>BufferingHints</b> object default values are used.</p>
      CompressionFormat?:
        type: CompressionFormat
        description: <p>The compression format. If no value is specified, the default is <code>NOCOMPRESSION</code>.</p> <p>The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot be specified for Amazon Redshift destinations because they are not supported by the Amazon Redshift <code>COPY</code> operation that reads from the S3 bucket.</p>
      EncryptionConfiguration?:
        type: EncryptionConfiguration
        description: <p>The encryption configuration. If no value is specified, the default is no encryption.</p>
      CloudWatchLoggingOptions?:
        type: CloudWatchLoggingOptions
        description: <p>Describes CloudWatch logging options for your delivery stream.</p>
    description: <p>Describes an update for a destination in Amazon S3.</p>
  UpdateDestinationInput:
    type: object
    properties:
      DeliveryStreamName:
        type: DeliveryStreamName
        description: <p>The name of the delivery stream.</p>
      CurrentDeliveryStreamVersionId:
        type: DeliveryStreamVersionId
        description: <p>Obtain this value from the <b>VersionId</b> result of the <a>DeliveryStreamDescription</a> operation. This value is required, and helps the service to perform conditional operations. For example, if there is a interleaving update and this value is null, then the update destination fails. After the update is successful, the <b>VersionId</b> value is updated. The service then performs a merge of the old configuration with the new configuration.</p>
      DestinationId:
        type: DestinationId
        description: <p>The ID of the destination.</p>
      S3DestinationUpdate?:
        type: S3DestinationUpdate
        description: <p>Describes an update for a destination in Amazon S3.</p>
      RedshiftDestinationUpdate?:
        type: RedshiftDestinationUpdate
        description: <p>Describes an update for a destination in Amazon Redshift.</p>
      ElasticsearchDestinationUpdate?:
        type: ElasticsearchDestinationUpdate
        description: <p>Describes an update for a destination in Amazon ES.</p>
    description: <p>Contains the parameters for <a>UpdateDestination</a>.</p>
  UpdateDestinationOutput:
    type: object
    description: <p>Contains the output of <a>UpdateDestination</a>.</p>
/{CreateDeliveryStream}:
  uriParameters:
    CreateDeliveryStream?:
      type: string
      enum:
      - ''
      (extras.syntetic): true
      description: This url parameter exists only to differentiate different operations on the same url
  displayName: Create Delivery Stream
  post:
    description: 'Creates a delivery stream.  <a>CreateDeliveryStream</a> is an asynchronous
      operation that immediately returns. The initial status of the delivery stream
      is <code>CREATING</code>. After the delivery stream is created, its status is
      <code>ACTIVE</code> and it now accepts data. Attempts to send data to a delivery
      stream that is not in the <code>ACTIVE</code> state cause an exception. To check
      the state of a delivery stream, use <a>DescribeDeliveryStream</a>. The name
      of a delivery stream identifies it. You can''t have two delivery streams with
      the same name in the same region. Two delivery streams in different AWS accounts
      or different regions in the same AWS account can have the same name. By default,
      you can create up to 20 delivery streams per region. A delivery stream can only
      be configured with a single destination, Amazon S3, Amazon Elasticsearch Service,
      or Amazon Redshift. For correct <a>CreateDeliveryStream</a> request syntax,
      specify only one destination configuration parameter: either <b>S3DestinationConfiguration</b>,
      <b>ElasticsearchDestinationConfiguration</b>, or <b>RedshiftDestinationConfiguration</b>.  As
      part of <b>S3DestinationConfiguration</b>, optional values <b>BufferingHints</b>,
      <b>EncryptionConfiguration</b>, and <b>CompressionFormat</b> can be provided.
      By default, if no <b>BufferingHints</b> value is provided, Firehose buffers
      data up to 5 MB or for 5 minutes, whichever condition is satisfied first. Note
      that <b>BufferingHints</b> is a hint, so there are some cases where the service
      cannot adhere to these conditions strictly; for example, record boundaries are
      such that the size is a little over or under the configured buffering size.
      By default, no encryption is performed. We strongly recommend that you enable
      encryption to ensure secure data storage in Amazon S3. A few notes about <b>RedshiftDestinationConfiguration</b>:
      <ul> <li> An Amazon Redshift destination requires an S3 bucket as intermediate
      location, as Firehose first delivers data to S3 and then uses <code>COPY</code>
      syntax to load data into an Amazon Redshift table. This is specified in the
      <b>RedshiftDestinationConfiguration.S3Configuration</b> parameter element. </li>
      <li> The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot
      be specified in <b>RedshiftDestinationConfiguration.S3Configuration</b> because
      the Amazon Redshift <code>COPY</code> operation that reads from the S3 bucket
      doesn''t support these compression formats. </li> <li> We strongly recommend
      that the username and password provided is used exclusively for Firehose purposes,
      and that the permissions for the account are restricted for Amazon Redshift
      <code>INSERT</code> permissions. </li> </ul> Firehose assumes the IAM role that
      is configured as part of destinations. The IAM role should allow the Firehose
      principal to assume the role, and the role should have permissions that allows
      the service to deliver the data. For more information, see <a href="http://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-s3">Amazon
      S3 Bucket Access</a> in the <i>Amazon Kinesis Firehose Developer Guide</i>.'
    displayName: Create Delivery Stream
    queryParameters:
      Action:
        type: string
        enum:
        - CreateDeliveryStream
    body:
      application/json: CreateDeliveryStreamInput
    responses:
      200:
        body:
          application/json: CreateDeliveryStreamOutput
      400:
        description: InvalidArgumentException
/{DeleteDeliveryStream}:
  uriParameters:
    DeleteDeliveryStream?:
      type: string
      enum:
      - ''
      (extras.syntetic): true
      description: This url parameter exists only to differentiate different operations on the same url
  displayName: Delete Delivery Stream
  post:
    description: Deletes a delivery stream and its data. You can delete a delivery stream only if it is in <code>ACTIVE</code> or <code>DELETING</code> state, and not in the <code>CREATING</code> state. While the deletion request is in process, the delivery stream is in the <code>DELETING</code> state. To check the state of a delivery stream, use <a>DescribeDeliveryStream</a>. While the delivery stream is <code>DELETING</code> state, the service may continue to accept the records, but the service doesn't make any guarantees with respect to delivering the data. Therefore, as a best practice, you should first stop any applications that are sending records before deleting a delivery stream.
    displayName: Delete Delivery Stream
    queryParameters:
      Action:
        type: string
        enum:
        - DeleteDeliveryStream
    body:
      application/json: DeleteDeliveryStreamInput
    responses:
      200:
        body:
          application/json: DeleteDeliveryStreamOutput
      400:
        description: ResourceInUseException
/{DescribeDeliveryStream}:
  uriParameters:
    DescribeDeliveryStream?:
      type: string
      enum:
      - ''
      (extras.syntetic): true
      description: This url parameter exists only to differentiate different operations on the same url
  displayName: Describe Delivery Stream
  post:
    description: Describes the specified delivery stream and gets the status. For example, after your delivery stream is created, call <a>DescribeDeliveryStream</a> to see if the delivery stream is <code>ACTIVE</code> and therefore ready for data to be sent to it.
    displayName: Describe Delivery Stream
    queryParameters:
      Action:
        type: string
        enum:
        - DescribeDeliveryStream
    body:
      application/json: DescribeDeliveryStreamInput
    responses:
      200:
        body:
          application/json: DescribeDeliveryStreamOutput
      400:
        description: ResourceNotFoundException
/{ListDeliveryStreams}:
  uriParameters:
    ListDeliveryStreams?:
      type: string
      enum:
      - ''
      (extras.syntetic): true
      description: This url parameter exists only to differentiate different operations on the same url
  displayName: List Delivery Streams
  post:
    description: Lists your delivery streams. The number of delivery streams might be too large to return using a single call to <a>ListDeliveryStreams</a>. You can limit the number of delivery streams returned, using the <b>Limit</b> parameter. To determine whether there are more delivery streams to list, check the value of <b>HasMoreDeliveryStreams</b> in the output. If there are more delivery streams to list, you can request them by specifying the name of the last delivery stream returned in the call in the <b>ExclusiveStartDeliveryStreamName</b> parameter of a subsequent call.
    displayName: List Delivery Streams
    queryParameters:
      Action:
        type: string
        enum:
        - ListDeliveryStreams
    body:
      application/json: ListDeliveryStreamsInput
    responses:
      200:
        body:
          application/json: ListDeliveryStreamsOutput
/{PutRecord}:
  uriParameters:
    PutRecord?:
      type: string
      enum:
      - ''
      (extras.syntetic): true
      description: This url parameter exists only to differentiate different operations on the same url
  displayName: Put Record
  post:
    description: Writes a single data record into an Amazon Kinesis Firehose delivery stream. To write multiple data records into a delivery stream, use <a>PutRecordBatch</a>. Applications using these operations are referred to as producers. By default, each delivery stream can take in up to 2,000 transactions per second, 5,000 records per second, or 5 MB per second. Note that if you use <a>PutRecord</a> and <a>PutRecordBatch</a>, the limits are an aggregate across these two operations for each delivery stream. For more information about limits and how to request an increase, see <a href="http://docs.aws.amazon.com/firehose/latest/dev/limits.html">Amazon Kinesis Firehose Limits</a>.  You must specify the name of the delivery stream and the data record when using <a>PutRecord</a>. The data record consists of a data blob that can be up to 1,000 KB in size, and any kind of data, for example, a segment from a log file, geographic location data, web site clickstream data, etc. Firehose buffers records before delivering them to the destination. To disambiguate the data blobs at the destination, a common solution is to use delimiters in the data, such as a newline (<code>\n</code>) or some other character unique within the data. This allows the consumer application(s) to parse individual data items when reading the data from the destination. The <a>PutRecord</a> operation returns a <b>RecordId</b>, which is a unique string assigned to each record. Producer applications can use this ID for purposes such as auditability and investigation. If the <a>PutRecord</a> operation throws a <b>ServiceUnavailableException</b>, back off and retry. If the exception persists, it is possible that the throughput limits have been exceeded for the delivery stream.  Data records sent to Firehose are stored for 24 hours from the time they are added to a delivery stream as it attempts to send the records to the destination. If the destination is unreachable for more than 24 hours, the data is no longer available.
    displayName: Put Record
    queryParameters:
      Action:
        type: string
        enum:
        - PutRecord
    body:
      application/json: PutRecordInput
    responses:
      200:
        body:
          application/json: PutRecordOutput
      400:
        description: ResourceNotFoundException
/{PutRecordBatch}:
  uriParameters:
    PutRecordBatch?:
      type: string
      enum:
      - ''
      (extras.syntetic): true
      description: This url parameter exists only to differentiate different operations on the same url
  displayName: Put Record Batch
  post:
    description: 'Writes multiple data records into a delivery stream in a single
      call, which can achieve higher throughput per producer than when writing single
      records. To write single data records into a delivery stream, use <a>PutRecord</a>.
      Applications using these operations are referred to as producers. Each <a>PutRecordBatch</a>
      request supports up to 500 records. Each record in the request can be as large
      as 1,000 KB (before 64-bit encoding), up to a limit of 4 MB for the entire request.
      By default, each delivery stream can take in up to 2,000 transactions per second,
      5,000 records per second, or 5 MB per second. Note that if you use <a>PutRecord</a>
      and <a>PutRecordBatch</a>, the limits are an aggregate across these two operations
      for each delivery stream. For more information about limits and how to request
      an increase, see <a href="http://docs.aws.amazon.com/firehose/latest/dev/limits.html">Amazon
      Kinesis Firehose Limits</a>.  You must specify the name of the delivery stream
      and the data record when using <a>PutRecord</a>. The data record consists of
      a data blob that can be up to 1,000 KB in size, and any kind of data, for example,
      a segment from a log file, geographic location data, web site clickstream data,
      and so on. Firehose buffers records before delivering them to the destination.
      To disambiguate the data blobs at the destination, a common solution is to use
      delimiters in the data, such as a newline (<code>\n</code>) or some other character
      unique within the data. This allows the consumer application(s) to parse individual
      data items when reading the data from the destination. The <a>PutRecordBatch</a>
      response includes a count of any failed records, <b>FailedPutCount</b>, and
      an array of responses, <b>RequestResponses</b>. The <b>FailedPutCount</b> value
      is a count of records that failed. Each entry in the <b>RequestResponses</b>
      array gives additional information of the processed record. Each entry in <b>RequestResponses</b>
      directly correlates with a record in the request array using the same ordering,
      from the top to the bottom of the request and response. <b>RequestResponses</b>
      always includes the same number of records as the request array. <b>RequestResponses</b>
      both successfully and unsuccessfully processed records. Firehose attempts to
      process all records in each <a>PutRecordBatch</a> request. A single record failure
      does not stop the processing of subsequent records. A successfully processed
      record includes a <b>RecordId</b> value, which is a unique value identified
      for the record. An unsuccessfully processed record includes <b>ErrorCode</b>
      and <b>ErrorMessage</b> values. <b>ErrorCode</b> reflects the type of error
      and is one of the following values: <code>ServiceUnavailable</code> or <code>InternalFailure</code>.
      <code>ErrorMessage</code> provides more detailed information about the error.
      If <b>FailedPutCount</b> is greater than 0 (zero), retry the request. A retry
      of the entire batch of records is possible; however, we strongly recommend that
      you inspect the entire response and resend only those records that failed processing.
      This minimizes duplicate records and also reduces the total bytes sent (and
      corresponding charges). If the <a>PutRecordBatch</a> operation throws a <b>ServiceUnavailableException</b>,
      back off and retry. If the exception persists, it is possible that the throughput
      limits have been exceeded for the delivery stream. Data records sent to Firehose
      are stored for 24 hours from the time they are added to a delivery stream as
      it attempts to send the records to the destination. If the destination is unreachable
      for more than 24 hours, the data is no longer available.'
    displayName: Put Record Batch
    queryParameters:
      Action:
        type: string
        enum:
        - PutRecordBatch
    body:
      application/json: PutRecordBatchInput
    responses:
      200:
        body:
          application/json: PutRecordBatchOutput
      400:
        description: ResourceNotFoundException
/{UpdateDestination}:
  uriParameters:
    UpdateDestination?:
      type: string
      enum:
      - ''
      (extras.syntetic): true
      description: This url parameter exists only to differentiate different operations on the same url
  displayName: Update Destination
  post:
    description: 'Updates the specified destination of the specified delivery stream.
      Note: Switching between Elasticsearch and other services is not supported. For
      Elasticsearch destination, you can only update an existing Elasticsearch destination
      with this operation. This operation can be used to change the destination type
      (for example, to replace the Amazon S3 destination with Amazon Redshift) or
      change the parameters associated with a given destination (for example, to change
      the bucket name of the Amazon S3 destination). The update may not occur immediately.
      The target delivery stream remains active while the configurations are updated,
      so data writes to the delivery stream can continue during this process. The
      updated configurations are normally effective within a few minutes. If the destination
      type is the same, Firehose merges the configuration parameters specified in
      the <a>UpdateDestination</a> request with the destination configuration that
      already exists on the delivery stream. If any of the parameters are not specified
      in the update request, then the existing configuration parameters are retained.
      For example, in the Amazon S3 destination, if <a>EncryptionConfiguration</a>
      is not specified then the existing <a>EncryptionConfiguration</a> is maintained
      on the destination. If the destination type is not the same, for example, changing
      the destination from Amazon S3 to Amazon Redshift, Firehose does not merge any
      parameters. In this case, all parameters must be specified. Firehose uses the
      <b>CurrentDeliveryStreamVersionId</b> to avoid race conditions and conflicting
      merges. This is a required field in every request and the service only updates
      the configuration if the existing configuration matches the <b>VersionId</b>.
      After the update is applied successfully, the <b>VersionId</b> is updated, which
      can be retrieved with the <a>DescribeDeliveryStream</a> operation. The new <b>VersionId</b>
      should be uses to set <b>CurrentDeliveryStreamVersionId</b> in the next <a>UpdateDestination</a>
      operation.'
    displayName: Update Destination
    queryParameters:
      Action:
        type: string
        enum:
        - UpdateDestination
    body:
      application/json: UpdateDestinationInput
    responses:
      200:
        body:
          application/json: UpdateDestinationOutput
      400:
        description: InvalidArgumentException
